{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base_lm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durham-abric/Comp551_Final_Proj/blob/master/base_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Wx1XADhOCTKL",
        "colab_type": "code",
        "outputId": "a4684fb6-07ec-4ca1-a309-b0701490013b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1QeGIErFv9xR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xtNpmqICWoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import concat\n",
        "from tensorflow.layers import Conv1D\n",
        "from tensorflow.layers import Dense\n",
        "from tensorflow.layers import Dropout\n",
        "from tensorflow.layers import Flatten\n",
        "from tensorflow.contrib import rnn\n",
        "from tensorflow.contrib.rnn import InputProjectionWrapper\n",
        "from tensorflow.nn import relu\n",
        "from tensorflow.nn import softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcUQGjragAD2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weights_init(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))\n",
        "  \n",
        "def bias_init(shape):\n",
        "    return tf.Variable(tf.zeros(shape=shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5_K7Y0Dwq4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def embedding_layer(vocab_size, embeding_size, inputs):\n",
        "  \n",
        "    word_embedings = tf.Variable(tf.random_uniform([vocab_size, embeding_size]))\n",
        "    embed = tf.nn.embedding_lookup(word_embedings, inputs)\n",
        "    embed_expended = tf.expand_dims(embed, -1) #expend dims to 4d for conv layer\n",
        "    return embed_expended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gWu1TaU-bfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def define_inputs(batch_size, sequence_len):\n",
        "  \n",
        "    inputs = tf.placeholder(tf.int32, [batch_size, sequence_len], name='inputs')\n",
        "    targets = tf.placeholder(tf.float32, [batch_size, 1], name='target')\n",
        "    keep_probs = tf.placeholder(tf.float32, name='keep_probs')\n",
        "    \n",
        "    return inputs, targets, keep_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtxM4TIznhX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_1d(input, kernel_size, number_of_filters, strides=(1, 1), activation=tf.nn.relu, max_pool=True):\n",
        "    weights = weights_init([kernel_size, kernel_size, 1, number_of_filters])\n",
        "    bias = bias_init([number_of_filters])\n",
        "  \n",
        "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n",
        "    \n",
        "    if activation != None:\n",
        "        layer = activation(layer)\n",
        "    \n",
        "    if max_pool:\n",
        "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "    \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MxwRHb8NiJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense(input, in_size, out_size, dropout=0.25, activation=tf.nn.relu):\n",
        "    weights = weights_init([in_size, out_size])\n",
        "    bias = bias_init([out_size])\n",
        "    \n",
        "    layer = tf.matmul(input, weights) + bias\n",
        "    \n",
        "    if activation != None:\n",
        "        layer = activation(layer)\n",
        "    \n",
        "    if dropout:\n",
        "        layer = tf.nn.dropout(layer, dropout)\n",
        "        \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GUKg0ZM0WD00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_layer(lstm_size, number_of_layers, batch_size, dropout_rate):\n",
        "  \n",
        "    def cell(size, dropout_rate=None):\n",
        "        layer = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        return tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
        "            \n",
        "    cell = tf.contrib.rnn.MultiRNNCell([cell(lstm_size, dropout_rate) for _ in range(number_of_layers)])\n",
        "    \n",
        "    init_state = cell.zero_state(batch_size, tf.float32)\n",
        "    return cell, init_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92FIXlfPWXNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "  \n",
        "  def __init__(self, learning_rate=0.02, dropout_rate=0.25, batch_size=128, project_dim = 1024, seq_len=50, vocab_size=1024, embed_size=300,\n",
        "               number_of_lstm_layers=2, lstm_units=128):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    self.inputs, self.targets, self.keep_probs = define_inputs(batch_size, seq_len)\n",
        "\n",
        "    word_embeddings = tf.get_variable('word_embeddings', [vocab_size, embed_size])\n",
        "    embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, self.inputs)\n",
        "\n",
        "    \n",
        "#     embeded_words = \n",
        "\n",
        "    # Neural network\n",
        "    # 8 conv-1d in parallel concatenated\n",
        "    conv_block_1 = tf.layers.conv1d(embedded_word_ids, 32, 1)\n",
        "#     conv_block_2 = tf.layers.conv1d(embedded_word_ids, 32, 2)\n",
        "#     conv_block_3 = tf.layers.conv1d(embedded_word_ids, 64, 3)\n",
        "#     conv_block_4 = tf.layers.conv1d(embedded_word_ids, 128, 4)\n",
        "#     conv_block_5 = tf.layers.conv1d(embedded_word_ids, 256, 5)\n",
        "#     conv_block_6 = tf.layers.conv1d(embedded_word_ids, 512, 6)\n",
        "#     conv_block_7 = tf.layers.conv1d(embedded_word_ids, 1028, 7)\n",
        "#     conv_block_8 = tf.layers.conv1d(embedded_word_ids, 2048, 7)\n",
        "    conv_layer = [conv_block_1]#, conv_block_2, conv_block_3,\n",
        "#                   conv_block_4, conv_block_5, conv_block_6,\n",
        "#                   conv_block_7, conv_block_8]\n",
        "    \n",
        "    concatenated_conv = tf.concat(conv_layer, 0)\n",
        "    \n",
        "    # Feed Forward Layers\n",
        "    logits_1 = tf.layers.dense(concatenated_conv, 4096)\n",
        "    logits_2 = tf.layers.dense(logits_1, 4096)\n",
        "    \n",
        "#     # Projection\n",
        "#     ## TODO\n",
        "    \n",
        "#     # LSTM Layers\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
        "    lstm_state = lstm.zero_state(batch_size, dtype=tf.float32)\n",
        "    logits_3, lstm_state = lstm_layer(logits_2, lstm_state, batch_size, dropout_rate)\n",
        "    logits_4, lstm_state = lstm_layer(logits_3, lstm_state, batch_size, dropout_rate)\n",
        "\n",
        "    \n",
        "#     # Predictions softmax\n",
        "    logits_5 = tf.layers.dense(logits_4, 800000)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GiIfOKaZO29I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "c80c3ed1-54da-4ebc-e1e5-58c0f4b1ac14"
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-86f71a50a8e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-2dfc57187e47>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate, dropout_rate, batch_size, project_dim, seq_len, vocab_size, embed_size, number_of_lstm_layers, lstm_units)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlstm_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlogits_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mlogits_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-02721cc99b68>\u001b[0m in \u001b[0;36mlstm_layer\u001b[0;34m(lstm_size, number_of_layers, batch_size, dropout_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropoutWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_keep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'LSTMStateTuple' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "m5jEHprXO7rh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}