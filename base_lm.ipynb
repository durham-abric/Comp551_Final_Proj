{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base_lm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durham-abric/Comp551_Final_Proj/blob/master/base_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Wx1XADhOCTKL",
        "colab_type": "code",
        "outputId": "7b5876f3-04f6-462b-d9d0-31e7ea5c7dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gCtRSTBLx3eu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "-xtNpmqICWoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import concat\n",
        "from tensorflow.layers import Conv1D\n",
        "from tensorflow.layers import Dense\n",
        "from tensorflow.layers import Dropout\n",
        "from tensorflow.layers import Flatten\n",
        "from tensorflow.contrib import rnn\n",
        "from tensorflow.contrib.rnn import InputProjectionWrapper\n",
        "from tensorflow.nn import relu\n",
        "from tensorflow.nn import softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcUQGjragAD2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weights_init(shape):\n",
        "  \"\"\"\n",
        "  Create a Tensor object of weights\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  shape : Tuple\n",
        "      Tuple describing the shape of the weights\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor\n",
        "      Returns a Tensor of shape specified with \n",
        "      normal distributed values in it\n",
        "      \n",
        "  \"\"\"\n",
        "  return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))\n",
        "  \n",
        "def bias_init(shape):\n",
        "  \"\"\"\n",
        "  Create a Tensor object of bias\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  shape : Tuple\n",
        "      Tuple describing the shape of the bias\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor\n",
        "      Returns a Tensor of shape specified with \n",
        "      normal distributed values in it\n",
        "\n",
        "  \"\"\"\n",
        "  return tf.Variable(tf.zeros(shape=shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gWu1TaU-bfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def define_inputs(batch_size, sequence_len, vocab_size):\n",
        "  \"\"\"\n",
        "  Create Tensor objects for inputs, tagets and probabilities\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch_size : int\n",
        "      Size of batch input\n",
        "  sequence_len : int\n",
        "      Maximum length of a sequence\n",
        "  vocab_size : int\n",
        "      Number of unique words in the vocabulary\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor, Tensor, Tensor\n",
        "      Returns 3 Tensors: inputs, targets and probabilities\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = tf.placeholder(tf.int32,\n",
        "                          [batch_size, sequence_len],\n",
        "                          name='inputs')\n",
        "  targets = tf.placeholder(tf.float32,\n",
        "                           [batch_size, vocab_size],\n",
        "                           name='target')\n",
        "  keep_probs = tf.placeholder(tf.float32,\n",
        "                              name='keep_probs')\n",
        "\n",
        "  return inputs, targets, keep_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtxM4TIznhX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_1d(input,\n",
        "            kernel_size,\n",
        "            number_of_filters,\n",
        "            strides=(1, 1),\n",
        "            activation=tf.nn.relu,\n",
        "            max_pool=True):\n",
        "  \"\"\"\n",
        "  Creates a convolution layer with custom parameters\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  input : Tensor\n",
        "      Input Tensor to the convolution layer\n",
        "  kernel_size : int\n",
        "      Size of the kernel of the convolution layer\n",
        "  number_of_filters : int\n",
        "      Number of filters in the convolution layer\n",
        "  strides : Tuple\n",
        "      Stride by the kernel in the convolution layer\n",
        "  activation : Tensor\n",
        "      Type of activation layer\n",
        "  max_pool : boolean\n",
        "      Activate a max pooling after the convolution\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor\n",
        "      Output of convolution layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  weights = weights_init([kernel_size, kernel_size, 1, number_of_filters])\n",
        "  bias = bias_init([number_of_filters])\n",
        "\n",
        "  layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n",
        "\n",
        "  if activation != None:\n",
        "      layer = activation(layer)\n",
        "\n",
        "  if max_pool:\n",
        "      layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MxwRHb8NiJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense(input,\n",
        "          in_size,\n",
        "          out_size,\n",
        "          dropout=0.25,\n",
        "          activation=tf.nn.relu):\n",
        "  \"\"\"\n",
        "  Creates a fully-connected layer with custom parameters\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  input : Tensor\n",
        "      Input Tensor to the dense layer\n",
        "  in_size : int\n",
        "      Size of the input\n",
        "  out_size : int\n",
        "      Size of the output\n",
        "  dropout : float\n",
        "      Rate of dropout applied to the output of the dense layer\n",
        "  activation : Tensor\n",
        "      Type of activation layer\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor\n",
        "      Output of dense layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  weights = weights_init([in_size, out_size])\n",
        "  bias = bias_init([out_size])\n",
        "\n",
        "  layer = tf.matmul(input, weights) + bias\n",
        "\n",
        "  if activation != None:\n",
        "      layer = activation(layer)\n",
        "\n",
        "  if dropout:\n",
        "      layer = tf.nn.dropout(layer, dropout)\n",
        "\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ft-YBAxVLuI1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def flatten_cnn(layers, shape):\n",
        "  \"\"\"\n",
        "  Flatten and concatenate a list of tensors\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  layers : list of Tenors\n",
        "    List of layers to concatenate\n",
        "  arg2 : int\n",
        "    First dimension of the output Tensor\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor\n",
        "    Returns a concatenated Tensor\n",
        "\n",
        "  \"\"\"\n",
        "  conv_layer_reshaped = []\n",
        "\n",
        "  for l in layers:\n",
        "    new_l = tf.reshape(l, [shape, -1])\n",
        "    conv_layer_reshaped.append(new_l) \n",
        "\n",
        "  return conv_layer_reshaped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKXMi_KIrLuh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_optimizer(logits, targets, learning_rate):\n",
        "  \"\"\"\n",
        "  Optimizer that implements the Adagrad algorithm.\n",
        "\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  logits : Tensor\n",
        "      Last layer output\n",
        "  targets : Tensor\n",
        "      True output of the function\n",
        "  learning_rate : float\n",
        "    Learning rate of the model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Tensor, Operation\n",
        "      Returns the loss tensor and an operation that update the model\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n",
        "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
        "  return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92FIXlfPWXNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "\n",
        "  def __init__(self, learning_rate=0.02,\n",
        "             dropout_rate=0.25,\n",
        "             batch_size=128,\n",
        "             seq_len=50,\n",
        "             vocab_size=1024,\n",
        "             embed_size=300,\n",
        "             layers_lstm=2):\n",
        "    \"\"\"\n",
        "    Creates a language model from arXiv:1806.02847v1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    learning_rate : float\n",
        "        Learning rate of the optimizer\n",
        "    dropout_rate : float\n",
        "        Dropout rate of the layers\n",
        "    batch_size : int\n",
        "        Size of the mini-batches\n",
        "    seq_len : int\n",
        "        Maximum length of an input\n",
        "    vocab_size : int\n",
        "        Size of the vocabulary\n",
        "    embed_size : int\n",
        "        Dimenions of the vocabulary embedding\n",
        "    layers_lstm : int\n",
        "        Number of layers of LSTM\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    self.dense_size = 4096\n",
        "    self.lstm_size = 8192\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_len = seq_len\n",
        "    self.inputs, self.targets, self.keep_probs = define_inputs(self.batch_size,\n",
        "                                                               self.seq_len,\n",
        "                                                               self.vocab_size)\n",
        "\n",
        "    # Neural network\n",
        "    # Embedding\n",
        "    word_embeddings = tf.get_variable('word_embeddings',\n",
        "                                      [vocab_size, embed_size])\n",
        "    embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, self.inputs)\n",
        "    self.embeddings = word_embeddings\n",
        "\n",
        "    # 8 conv-1d in parallel concatenated\n",
        "    conv_block_1 = tf.layers.conv1d(embedded_word_ids, 32, 1)\n",
        "    conv_block_2 = tf.layers.conv1d(embedded_word_ids, 32, 2)\n",
        "    conv_block_3 = tf.layers.conv1d(embedded_word_ids, 64, 3)\n",
        "    conv_block_4 = tf.layers.conv1d(embedded_word_ids, 128, 4)\n",
        "    conv_block_5 = tf.layers.conv1d(embedded_word_ids, 256, 5)\n",
        "    conv_block_6 = tf.layers.conv1d(embedded_word_ids, 512, 6)\n",
        "    conv_block_7 = tf.layers.conv1d(embedded_word_ids, 1028, 7)\n",
        "    conv_block_8 = tf.layers.conv1d(embedded_word_ids, 2048, 7)\n",
        "\n",
        "    conv_layer = [conv_block_1, conv_block_2, conv_block_3,\n",
        "                  conv_block_4, conv_block_5, conv_block_6,\n",
        "                  conv_block_7, conv_block_8]\n",
        "\n",
        "    conv_layer = flatten_cnn(conv_layer, batch_size)\n",
        "    concatenated_conv = tf.concat(conv_layer, 1)\n",
        "\n",
        "    # Feed Forward Layers\n",
        "    logits = tf.layers.dense(concatenated_conv, self.dense_size)\n",
        "    logits = tf.layers.dense(logits, self.dense_size)\n",
        "    logits = tf.reshape(logits, [logits.get_shape()[0],\n",
        "                                 logits.get_shape()[1], 1])\n",
        "\n",
        "    # LSTM Layers\n",
        "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(s) for s in [self.lstm_size] * layers_lstm]\n",
        "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
        "    logits, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
        "                                      inputs=logits,\n",
        "                                      dtype=tf.float32)\n",
        "\n",
        "    # Predictions softmax\n",
        "    logits = tf.reshape(logits, [logits.get_shape()[0], -1])\n",
        "    logits = tf.layers.dense(logits, vocab_size)\n",
        "    softmax = tf.nn.softmax(logits)\n",
        "\n",
        "    self.loss, self.opt = loss_optimizer(softmax, self.targets, learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GiIfOKaZO29I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5jEHprXO7rh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jD99EEqrfSZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJhEpJjEx7qx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "bpYRvMLwyqn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZR_8HAPbx-Iw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_vocab = 'gdrive/My Drive/Mcgill/U4/Fall 2018/COMP 551/assignment/COMP551/data/vocab.txt'\n",
        "path_to_pdp60 = 'gdrive/My Drive/Mcgill/U4/Fall 2018/COMP 551/assignment/COMP551/data/pdp60.json'\n",
        "path_to_wsc273 = 'gdrive/My Drive/Mcgill/U4/Fall 2018/COMP 551/assignment/COMP551/data/wsc273.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4CkJLYZx-OT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_file = [line.rstrip('\\n') for line in open(path_to_vocab)]\n",
        "number_of_words = len(vocab_file)\n",
        "print(\"The vocabulary contains a total of %s words.\" % number_of_words)\n",
        "\n",
        "words_pdp = None\n",
        "words_wsc = None\n",
        "with open(path_to_pdp60) as f:\n",
        "    words_pdp = json.load(f)\n",
        "with open(path_to_wsc273) as f:\n",
        "    words_wsc = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBmByZjVy2G1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kK8yoLgYyG-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vocabulary\n",
        "Get files the pdp60 and wsc273 and compare with the original size. Share same words but reducethe number of words in the vocab."
      ]
    },
    {
      "metadata": {
        "id": "La8VePnvyJnl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_str(e):\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  normalizer = lambda x: x.lower().translate(translator) # lower and remove punctuation\n",
        "  return list(filter(None, list(map(normalizer, e.split(' ')))))\n",
        "  \n",
        "class Vocabulary():\n",
        "  def __init__(self):\n",
        "    self.size = 0\n",
        "    self.n_percent = None\n",
        "    self.max_size = None\n",
        "    self.words = {}\n",
        "    self.rejected = []\n",
        "    \n",
        "\n",
        "  def add_to_vocab(self, vocab):\n",
        "    # iterate through words add add if not\n",
        "    # in words list and add occurence anyways\n",
        "    for w in vocab:\n",
        "      try:\n",
        "        w = normalize_str(w)[0]\n",
        "        if w in vocab_file:\n",
        "          if w in self.words:\n",
        "            self.words[w] += 1\n",
        "          else:\n",
        "            self.size += 1\n",
        "            self.words[w] = 1\n",
        "        else:\n",
        "          if w in self.rejected:\n",
        "            pass\n",
        "          else:\n",
        "            print(\"The word '%s' is not in the vocab.txt file. '%s' is not added to the dictionary.\" % (w, w))\n",
        "            self.rejected.append(w)\n",
        "      except:\n",
        "        pass\n",
        "    return\n",
        "  \n",
        "      \n",
        "  def get_vocab(self):\n",
        "    # Make sure max_size is set and return n most frequent words.\n",
        "    if self.max_size is None:\n",
        "        print(\"Maximum size set to None. Update the size with set_max_size(n).\")\n",
        "        return\n",
        "    print(\"Returning %s words.\" % self.max_size)\n",
        "    n_sorted_words = (sorted(self.words.items(), key=lambda kv: kv[1], reverse = True)[:self.max_size])\n",
        "    return np.array([k for k,v in n_sorted_words])\n",
        "  \n",
        "  \n",
        "  def set_max_size(self, n):\n",
        "    if n == self.max_size:\n",
        "      return\n",
        "    elif n == -1:\n",
        "      print(\"Updating the size of the vocab from '%s' to '%s'. Keeping all the words in the vocabulary.\" % (self.max_size, int(len(self.words))))\n",
        "      self.max_size = len(self.words)\n",
        "    elif n < 1:\n",
        "      total = int(len(self.words)*n)\n",
        "      print(\"Updating the size of the vocab from '%s' to '%s'.\" % (self.max_size, total))\n",
        "      self.max_size = total\n",
        "    else:\n",
        "      print(\"Updating the size of the vocab from '%s' to '%s'.\" % (self.max_size, n))\n",
        "      self.max_size = n\n",
        "    return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yURSvr_IyJq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_obj = Vocabulary()\n",
        "\n",
        "for s in words_wsc:\n",
        "  vocab_obj.add_to_vocab(s['substitution'].split(\" \"))\n",
        "for s in words_pdp:\n",
        "  vocab_obj.add_to_vocab(s['substitution'].split(\" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZ1n0loHyPzx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_obj.set_max_size(-1)\n",
        "X_train = vocab_obj.get_vocab()\n",
        "y_train = vocab_obj.get_vocab()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sa_agSs1xrLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGN-Y-OmfUIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "    epoch_loss = []\n",
        "    train_accuracy = []\n",
        "    for ii in (range(0, len(X_train), batch_size)):\n",
        "        X_batch = X_train[ii:ii+batch_size]\n",
        "        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n",
        "\n",
        "        c, _ = session.run([model.loss, model.opt],\n",
        "                              feed_dict={model.inputs:X_batch, \n",
        "                              model.targets:y_batch})\n",
        "        \n",
        "        epoch_loss.append(c)\n",
        "        \n",
        "    \n",
        "    print(\"Epoch: {}/{}\".format(i, epochs),\n",
        "          \" | Epoch loss: {}\".format(np.mean(epoch_loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHGYKL4kxorn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}